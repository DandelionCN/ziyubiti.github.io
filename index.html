<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>ziyubiti | study hard everyday</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">ziyubiti</h1><a id="logo" href="/.">ziyubiti</a><p class="description">study hard everyday</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/categories/ML/"><i class="fa undefined"> ML</i></a><a href="/categories/Python/"><i class="fa undefined"> Python</i></a><a href="/categories/Web/"><i class="fa undefined"> Web</i></a><a href="/categories/5G/"><i class="fa undefined"> 5G</i></a><a href="/categories/Funny/"><i class="fa undefined"> Funny</i></a><a href="/archives/"><i class="fa undefined"> Archive</i></a><a href="/about/"><i class="fa undefined"> About</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h2 class="post-title"><a href="/2016/12/11/nnzoo/">神经网络架构大盘点--读Fjodor Van Veen的《neural-network-zoo》</a></h2><div class="post-meta">2016-12-11</div><a data-thread-key="2016/12/11/nnzoo/" href="/2016/12/11/nnzoo/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　各种神经网络架构层出不穷，比如DCIGN, BiLSTM, DCGAN等，Fjodor Van Veen总结了近几年来流行的神经网络各架构，易于直观理解。<br>　　原文地址为：<a href="http://www.asimovinstitute.org/neural-network-zoo/" target="_blank" rel="external">http://www.asimovinstitute.org/neural-network-zoo/</a><br>　　总结如下：<br>　　<img src="../../../../imgs/nnzoo/neuralnetworks.png" alt="neural network architectures">  </p></div><p class="readmore"><a href="/2016/12/11/nnzoo/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/12/05/tradespider/">爬虫抓取建委数据--观察存量房成交量变化趋势</a></h2><div class="post-meta">2016-12-05</div><a data-thread-key="2016/12/05/tradespider/" href="/2016/12/05/tradespider/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　成交量是市场行情变化的一个重要指标，量在价先，价格的变化必然首先通过成交量的变化体现出来。根据价格与成交量的数据变化趋势，从而选择合适的存量房交易时机，从而尽可能减小市场风险。<br>　　因建委网站每天会公布存量房交易量，那么我们就可以做个小爬虫，每天自动抓取其数据，从而可以观察其变化趋势，例如日线、周线、月线等。<br>　　设计思路如下：<br>　　1、数据库中建立三个表，分别存储每日、每周、每月的存量房成交量数据；<br>　　2、后台每天自动运行爬虫程序，抓取数据存入数据库；<br>　　3、前端从数据库读取数据，绘制图形曲线，便于可视化观察趋势。<br>　　<br>　　完整代码见<a href="https://github.com/ziyubiti/jianweiSpider" target="_blank" rel="external">我的github</a>，图形结果如下：<br>　　<img src="../../../../imgs/tradespider/daily_trades.png" alt="每日成交量数据"><br>　　<img src="../../../../imgs/tradespider/weekly_trades.png" alt="每周成交量数据"><br>　　<img src="../../../../imgs/tradespider/month_trades.png" alt="每月成交量数据"><br>　　<br>　　对于北京市场而言，可以看出：<br>　　1、日成交量在800以上，属于上涨趋势；1200以上，属于快速上涨趋势；400以下，属于下跌趋势；400-800之间，平稳趋势。<br>　　2、周成交量在4000以上，属于上涨趋势；6000以上，属于快速上涨趋势；2000以下，属于下跌趋势；2000-4000之间，平稳趋势。<br>　　3、月成交量在15000以上，属于上涨趋势；20000以上，属于快速上涨趋势；10000以下，属于下跌趋势；10000-15000之间，平稳趋势。   </p></div><p class="readmore"><a href="/2016/12/05/tradespider/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/11/27/cnnnet/">CNN经典网络模型摘要--AlexNet、ZFnet、GoogleNet、VGG、ResNet</a></h2><div class="post-meta">2016-11-27</div><a data-thread-key="2016/11/27/cnnnet/" href="/2016/11/27/cnnnet/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　CNN的经典结构始于1998年的LeNet，成于2012年历史性的AlexNet，从此大盛于图像相关领域，主要包括：<br>　　1、LeNet，1998年<br>　　2、AlexNet，2012年<br>　　3、ZF-net，2013年<br>　　4、GoogleNet，2014年<br>　　5、VGG，2014年<br>　　6、ResNet，2015年  </p></div><p class="readmore"><a href="/2016/11/27/cnnnet/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/11/20/cnnpara/">神经网络参数优化--基于CNN的验证</a></h2><div class="post-meta">2016-11-20</div><a data-thread-key="2016/11/20/cnnpara/" href="/2016/11/20/cnnpara/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　当使用多层更深的隐藏层全连接网络时，参数量会变得非常巨大，达到数十亿量级；而采用CNN结构，则可以层间共享权重，极大减小待训练的参数量；同时可采用二维卷积，保留图像的空间结构信息；采用池化层，进一步减少参数计算。<br>　　一般来说，提高泛化能力的方法主要有：    正则化、增加神经网络层数、改变激活函数与代价函数、使用好的权重初始化技术、人为扩展训练集、弃权技术。<br>　　下面以MNIST为例，结合CNN、Pooling、Fc结构，通过不同的网络结构变化，给这些参数优化理论一个直观的验证结果。<br><img src="../../../../imgs/cnnpara/cnnpara1.png" alt=""><br><img src="../../../../imgs/cnnpara/cnnpara2.png" alt="CNN不同网络结构性能比较"></p></div><p class="readmore"><a href="/2016/11/20/cnnpara/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/11/13/googlenet-inception/">为什么GoogleNet中的Inception Module使用1*1 convolutions?</a></h2><div class="post-meta">2016-11-13</div><a data-thread-key="2016/11/13/googlenet-inception/" href="/2016/11/13/googlenet-inception/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　GoogleNet的创新之处，在于引入了Inception Module，其采用了并行结构，如下：<br>　　<img src="../../../../imgs/googlenet/inception.png" alt="GoogleNet Inception Module"><br>　　<br>　　其中1*1 conv的作用：<br>　　简单说，是特征降维，是feature pooling，filter space的transform。这种跨特征层的级联结构，可以有助于不同特征层间的空间信息交互。论文中语:”This cascaded cross channel parameteric pooling structure allows complex and learnable interactions of cross channel information”。<br>　　这篇文章的解释：<a href="http://iamaaditya.github.io/2016/03/one-by-one-convolution/" target="_blank" rel="external">http://iamaaditya.github.io/2016/03/one-by-one-convolution/</a><br>　　youtube上的1x1 Convolutions/inception相关视频，如下：<br>　　<video width="640" height="360" src="../../../../imgs/googlenet/1x1 Convolutions.mp4" autoplay loop controls="controls"></video><br>　　<video width="640" height="360" src="../../../../imgs/googlenet/Inception Module.mp4" autoplay loop controls="controls"></video></p></div><p class="readmore"><a href="/2016/11/13/googlenet-inception/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/11/06/gradvanish/">深度神经网络的梯度不稳定问题--梯度消失与梯度爆炸</a></h2><div class="post-meta">2016-11-06</div><a data-thread-key="2016/11/06/gradvanish/" href="/2016/11/06/gradvanish/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　深度神经网络中的梯度不稳定性，前面层中的梯度或会消失，或会爆炸。  </p></div><p class="readmore"><a href="/2016/11/06/gradvanish/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/11/01/actfunpaper/">神经网络不同激活函数比较--读《Understanding the difficulty of training deep feedforward neural networks》</a></h2><div class="post-meta">2016-11-01</div><a data-thread-key="2016/11/01/actfunpaper/" href="/2016/11/01/actfunpaper/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　<img src="../../../../imgs/nnactfun/1.png" alt="various activation functions performance"><br>　　这篇论文比较了不同激活函数的中间各隐层输出值分布、梯度值分布、测试准确度等，从上图看出，至少在mnist和cifar10测试集下，看起来tanh和softsign都要好于sigmoid。</p></div><p class="readmore"><a href="/2016/11/01/actfunpaper/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/10/24/nnpara/">神经网络参数优化</a></h2><div class="post-meta">2016-10-24</div><a data-thread-key="2016/10/24/nnpara/" href="/2016/10/24/nnpara/#comments" class="ds-thread-count"></a><div class="post-content"><h2 id="1-学习缓慢问题"><a href="#1-学习缓慢问题" class="headerlink" title=" 1.学习缓慢问题"></a> <strong>1.学习缓慢问题</strong></h2><p>　　激活函数采用sigmoid，代价函数采用二次项时，会存在学习缓慢即梯度下降较慢的问题。对此，可以使用交叉熵代价函数，或者使用ReLU、Softmax激活函数等，如下图。　　　　<br>　　<img src="../../../../imgs/nnpara/learningslow.png" alt="">  </p></div><p class="readmore"><a href="/2016/10/24/nnpara/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/10/18/mlppaper/">回归初心-读《Deep Big Simple Neural Nets Excel on Hand-written Digit Recognition》</a></h2><div class="post-meta">2016-10-18</div><a data-thread-key="2016/10/18/mlppaper/" href="/2016/10/18/mlppaper/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　Mnist做为手写数字识别的基准数据集，one Hidden Layer Perceptron已可以取得较好性能（Error rate 2%-5%），更有诸多新型网络结构如CNN、RNN不断改善性能（Error rate 1%以下）。<br>　　而这篇论文，则进一步回归初心，挖掘纯粹MLP的性能，通过可变学习速率，旋转、畸变数据以扩展训练集，更多隐藏层等，观察性能如下：<br>　　<img src="../../../../imgs/MLP/MLP.png" alt="MLP performance"><br>　　<br>　　没有花拳绣腿，纯粹的MLP，同样可以得到错误率0.5%以下的性能，唯一的缺点就是计算量，方法上简单、粗暴、有效！</p></div><p class="readmore"><a href="/2016/10/18/mlppaper/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/10/11/datascientistway/">数据科学家路线图</a></h2><div class="post-meta">2016-10-11</div><a data-thread-key="2016/10/11/datascientistway/" href="/2016/10/11/datascientistway/#comments" class="ds-thread-count"></a><div class="post-content"><p>网上搜到的，感谢原作者。<br>　　<img src="../../../../imgs/datascientistway/1.jpg" alt="">  </p></div><p class="readmore"><a href="/2016/10/11/datascientistway/">Read More</a></p></div><nav class="page-navigator"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next</a></nav></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/5G/">5G</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Funny/">Funny</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life/">Life</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML/">ML</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Web/">Web</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Funny/" style="font-size: 15px;">Funny</a> <a href="/tags/LTE/" style="font-size: 15px;">LTE</a> <a href="/tags/NB-IoT/" style="font-size: 15px;">NB-IoT</a> <a href="/tags/MachineLearning/" style="font-size: 15px;">MachineLearning</a> <a href="/tags/DeepLearning/" style="font-size: 15px;">DeepLearning</a> <a href="/tags/crawl/" style="font-size: 15px;">crawl</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Crab/" style="font-size: 15px;">Crab</a> <a href="/tags/Life/" style="font-size: 15px;">Life</a> <a href="/tags/RecSys/" style="font-size: 15px;">RecSys</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/12/11/nnzoo/">神经网络架构大盘点--读Fjodor Van Veen的《neural-network-zoo》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/05/tradespider/">爬虫抓取建委数据--观察存量房成交量变化趋势</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/27/cnnnet/">CNN经典网络模型摘要--AlexNet、ZFnet、GoogleNet、VGG、ResNet</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/20/cnnpara/">神经网络参数优化--基于CNN的验证</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/13/googlenet-inception/">为什么GoogleNet中的Inception Module使用1*1 convolutions?</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/06/gradvanish/">深度神经网络的梯度不稳定问题--梯度消失与梯度爆炸</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/01/actfunpaper/">神经网络不同激活函数比较--读《Understanding the difficulty of training deep feedforward neural networks》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/24/nnpara/">神经网络参数优化</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/18/mlppaper/">回归初心-读《Deep Big Simple Neural Nets Excel on Hand-written Digit Recognition》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/11/datascientistway/">数据科学家路线图</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> Recent Comments</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="http://blog.csdn.net/ziyubiti" title="ziyubiti's csdn blog" target="_blank">ziyubiti's csdn blog</a><ul></ul><a href="https://github.com/ziyubiti" title="ziyubiti's github repository" target="_blank">ziyubiti's github repository</a><ul></ul><a href="null" title="null" target="_blank"></a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">ziyubiti 2006-2017.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a> Theme by<a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> maupassant.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'ziyubiti'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>