<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>ziyubiti | study hard everyday</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">ziyubiti</h1><a id="logo" href="/.">ziyubiti</a><p class="description">study hard everyday</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/categories/ML/"><i class="fa undefined"> ML</i></a><a href="/categories/Python/"><i class="fa undefined"> Python</i></a><a href="/categories/Web/"><i class="fa undefined"> Web</i></a><a href="/categories/5G/"><i class="fa undefined"> 5G</i></a><a href="/categories/Funny/"><i class="fa undefined"> Funny</i></a><a href="/archives/"><i class="fa undefined"> Archive</i></a><a href="/about/"><i class="fa undefined"> About</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h2 class="post-title"><a href="/2016/10/08/lenetintro/">LeNet简介</a></h2><div class="post-meta">2016-10-08</div><a data-thread-key="2016/10/08/lenetintro/" href="/2016/10/08/lenetintro/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　LeNet做为CNN的经典网络结构，如下。<br>　　<img src="../../../../imgs/lenet/lenet1.png" alt="LeNet-5架构"><br>　　<img src="../../../../imgs/lenet/lenet2.png" alt="S2到C3的映射"><br>　　<img src="../../../../imgs/lenet/lenet_para.png" alt="LeNet-5参数量">  </p></div><p class="readmore"><a href="/2016/10/08/lenetintro/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/09/30/improvedlperformance/">如何改进深度学习的性能？</a></h2><div class="post-meta">2016-09-30</div><a data-thread-key="2016/09/30/improvedlperformance/" href="/2016/09/30/improvedlperformance/#comments" class="ds-thread-count"></a><div class="post-content"><p>摘自：<a href="http://machinelearningmastery.com/improve-deep-learning-performance/" target="_blank" rel="external">How To Improve Deep Learning Performance</a>  </p></div><p class="readmore"><a href="/2016/09/30/improvedlperformance/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/09/22/lianjiaML/">链家大数据使用到的机器学习算法</a></h2><div class="post-meta">2016-09-22</div><a data-thread-key="2016/09/22/lianjiaML/" href="/2016/09/22/lianjiaML/#comments" class="ds-thread-count"></a><div class="post-content"><p>摘自: <a href="http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2650994254&amp;idx=2&amp;sn=f591ab2d1a15a500fc1f12b6d55c52e9&amp;chksm=bdbf0e1d8ac8870be98a3f0eff315f2e9d74b079601eec23ed472f54cabc1310a13e7885a5b0&amp;scene=0#wechat_redirect" target="_blank" rel="external">InfoQ对链家网大数据架构师蔡白银的访谈</a>  </p></div><p class="readmore"><a href="/2016/09/22/lianjiaML/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/09/18/BPbrief/">BP算法摘要与代码分析</a></h2><div class="post-meta">2016-09-18</div><a data-thread-key="2016/09/18/BPbrief/" href="/2016/09/18/BPbrief/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　反向传播算法是神经网络中的重要组成部分，是对权重和偏置变化影响代价函数过程的理解，目标是计算代价函数C 分别关于w 和b的偏导数，从而更新w、b，影响前向过程，最小化代价函数。<br>　　<img src="../../../../imgs/BP/0.png" alt="神经网络结构示意图"><br>　　<img src="../../../../imgs/BP/1.png" alt="BP算法的4个方程"><br>　　公式BP1与BP2在nielsen的书<a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">Neural Networks and Deep Learning</a>中已有证明，下面补充BP3与BP4的证明：<br>　　<img src="../../../../imgs/BP/2.png" alt="BP3 and BP4 Proof"><br>　　BP算法过程如下：<br>　　<img src="../../../../imgs/BP/3.png" alt="BP算法过程"><br>　　在m大小的mini_batch内进行一次梯度更新，过程如下：<br>　　<img src="../../../../imgs/BP/4.png" alt="mini_batch内的一次梯度更新">    　　<br>　　<br>　　以mnist识别，三层网络[784,30,10]为例，nielsen的书中代码如下，添加了部分注释：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></div><div class="line"></div><div class="line">        <span class="string">"""Update the network's weights and biases by applying</span></div><div class="line"></div><div class="line">        gradient descent using backpropagation to a single mini batch.</div><div class="line"></div><div class="line">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</div><div class="line"></div><div class="line">        is the learning rate."""</div><div class="line"></div><div class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]    <span class="comment"># idx0:30*1; idx1:10*1</span></div><div class="line"></div><div class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]   <span class="comment"># idx0:30*784; idx1:10*30</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:                              <span class="comment"># loop 10, BP calc  and update gradient 10 times</span></div><div class="line"></div><div class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)        <span class="comment"># delta b,delta w , their dimension is same as b,w</span></div><div class="line"></div><div class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]  <span class="comment"># bj:zip two column vector, but list output row by row</span></div><div class="line"></div><div class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</div><div class="line"></div><div class="line">        self.weights = [w-(eta/len(mini_batch))*nw</div><div class="line"></div><div class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]     <span class="comment"># update w, sum the average 10 changes</span></div><div class="line"></div><div class="line">        self.biases = [b-(eta/len(mini_batch))*nb</div><div class="line"></div><div class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]       <span class="comment"># update b,</span></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></div><div class="line"></div><div class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></div><div class="line"></div><div class="line">        gradient for the cost function C_x.  ``nabla_b`` and</div><div class="line"></div><div class="line">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</div><div class="line"></div><div class="line">        to ``self.biases`` and ``self.weights``."""</div><div class="line"></div><div class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</div><div class="line"></div><div class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</div><div class="line"></div><div class="line">        <span class="comment"># feedforward</span></div><div class="line"></div><div class="line">        activation = x</div><div class="line"></div><div class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></div><div class="line"></div><div class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</div><div class="line"></div><div class="line">            z = np.dot(w, activation)+b</div><div class="line"></div><div class="line">            zs.append(z)                              <span class="comment"># middle variable z before sigmoid, z1,z2</span></div><div class="line"></div><div class="line">            activation = sigmoid(z)</div><div class="line"></div><div class="line">            activations.append(activation)            <span class="comment"># activation output, x, s1,s2</span></div><div class="line"></div><div class="line">        <span class="comment"># backward pass</span></div><div class="line"></div><div class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \                <span class="comment"># C=1/2 *(s2-y).^2, so dc/ds=(s2-y)</span></div><div class="line"></div><div class="line">            sigmoid_prime(zs[<span class="number">-1</span>])                                            <span class="comment"># delta 10*1</span></div><div class="line"></div><div class="line">        nabla_b[<span class="number">-1</span>] = delta</div><div class="line"></div><div class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())           <span class="comment"># s1: 30*1, s1':1*30; delta:10*1, dot:10*30</span></div><div class="line"></div><div class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></div><div class="line"></div><div class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></div><div class="line"></div><div class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></div><div class="line"></div><div class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></div><div class="line"></div><div class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></div><div class="line"></div><div class="line">        <span class="comment"># that Python can use negative indices in lists.</span></div><div class="line"></div><div class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</div><div class="line"></div><div class="line">            z = zs[-l]                                                         </div><div class="line"></div><div class="line">            sp = sigmoid_prime(z)                                           <span class="comment">#sp1: dz1, 30*1</span></div><div class="line"></div><div class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp       <span class="comment"># w[1]: 10*30;  delta update to 30*1 </span></div><div class="line"></div><div class="line">            nabla_b[-l] = delta</div><div class="line"></div><div class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())      <span class="comment"># delta 30*1, l=2, s0=x,784*1, ze w[-2]=w[0]=30*784 </span></div><div class="line"></div><div class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></div><div class="line"></div><div class="line">        <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></div><div class="line"></div><div class="line">        \partial a for the output activations."""</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (output_activations-y)</div></pre></td></tr></table></figure></p></div><p class="readmore"><a href="/2016/09/18/BPbrief/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/09/08/deepdreamintro/">Deep Dream 初体验 - 神经网络模型眼中的世界</a></h2><div class="post-meta">2016-09-08</div><a data-thread-key="2016/09/08/deepdreamintro/" href="/2016/09/08/deepdreamintro/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　2015年年中Google发布了Deep Dream的新闻，展示了google神经网络模型对输入图片的理解，类似于“深度盗梦”，并引发了后续的艺术风格作画。<br>　　Deep Dream，其原理并不是尽可能地去正确识别图片对象，而是在网络识别对象模式的中间过程时，忽略确认操作，这样就解除了最小梯度限制，Test loss不再越来越小；而是让网络在已识别出的错误对象上，继续去强化认知和理解，最终找出了新图像的模板，而这迥异于原图的风格，形成了类似于创造梦境的效果。<br>　　Google的示例如下：<br>　　<img src="../../../../imgs/deepdream/googledeepdream.jpg" alt="google deep dream example 1"><br>　　<img src="../../../../imgs/deepdream/googledeepdream2.jpg" alt="google deep dream example 2 - 原图"><br>　　<img src="../../../../imgs/deepdream/googledeepdream3.jpg" alt="google deep dream example 2 - 转换图"><br>　　<img src="../../../../imgs/deepdream/googledeepdream4.jpg" alt="google deep dream example 3 - 原图"><br>　　<img src="../../../../imgs/deepdream/googledeepdream5.jpg" alt="google deep dream example 3 - 转换图">  　　</p></div><p class="readmore"><a href="/2016/09/08/deepdreamintro/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/08/30/awesomeML/">机器学习相关的Awesome系列</a></h2><div class="post-meta">2016-08-30</div><a data-thread-key="2016/08/30/awesomeML/" href="/2016/08/30/awesomeML/#comments" class="ds-thread-count"></a><div class="post-content"><p></p><p>原文链接：<a href="https://www.52ml.net/17812.html" target="_blank" rel="nofollow">[原创]机器学习相关的Awesome系列</a></p><p></p></div><p class="readmore"><a href="/2016/08/30/awesomeML/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/08/20/neural-style/">Tensorflow Neural Network初体验 - 让电脑自己学习艺术风格去处理图片</a></h2><div class="post-meta">2016-08-20</div><a data-thread-key="2016/08/20/neural-style/" href="/2016/08/20/neural-style/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　最近俄国的一款APP比较火，名叫Prisma，利用人工神经网络技术，可以学习名画艺术风格，对我们的照片进行艺术化处理。<br>　　好奇心起，了解了一下其原理，简单说即学习一副画的风格，并将这幅画的风格应用到另一幅图片上。那我们完全可以利用Tensorflow Neural Network功能，来实现类似Prisma的功能，可以自定义诸多参数，比如多幅画的混合风格，不同权重，学习率等，来任意定制我们想要的绘画风格，取得比Prisma更灵活的效果。<br>　　利用神经网络进行人工智能绘画，起源于2015年9月的一篇论文”A Neural Algorithm of Artistic Style”，如下图显示不同风格的处理效果。<br>　　<img src="../../../../imgs/NN_artist/NNart.jpg" alt="A Neural Algorithm of Artistic Style"><br>　　代码参考<a href="https://github.com/anishathalye/neural-style" target="_blank" rel="external">anishathalye/neural-style</a>,基于Tensorflow CNN工具包和VGG模型。VGG做为常用的几种模型结构，与AlexNet类似，如下图示意：<br>　　<img src="../../../../imgs/NN_artist/VGG.jpg" alt="VGG model"><br>　　转换图片风格命令如下：<br>   <code>python neural_style.py --content &lt;content file&gt; --styles &lt;style file&gt; --output &lt;output file&gt;</code></p></div><p class="readmore"><a href="/2016/08/20/neural-style/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/08/16/mysql-python-exception/">MySQL Connection not available的定位</a></h2><div class="post-meta">2016-08-16</div><a data-thread-key="2016/08/16/mysql-python-exception/" href="/2016/08/16/mysql-python-exception/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　Python在读写远程MySql数据库时，在数据量略大时（约4000条），总是运行一段时间后，过程中出现：OperationalError: MySQL Connection not available，查看代码与cursor = conn.cursor()有关。<br>　　网上查了些资料，与MySql的固有bug有关（<a href="http://bugs.mysql.com/bug.php?id=67649）：" target="_blank" rel="external">http://bugs.mysql.com/bug.php?id=67649）：</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Description:</div><div class="line">If you open an unbuffered cursor and you do not read the WHOLE result set before closing the cursor, the next query will fail with</div><div class="line"></div><div class="line">  File &quot;/usr/lib/python3.2/site-packages/mysql/connector/connection.py&quot;, line 1075, in cursor</div><div class="line">    raise errors.OperationalError(&quot;MySQL Connection not available.&quot;)</div><div class="line">mysql.connector.errors.OperationalError: MySQL Connection not available.</div><div class="line"></div><div class="line">and all subsequent database calls will fail too.</div></pre></td></tr></table></figure></p></div><p class="readmore"><a href="/2016/08/16/mysql-python-exception/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/08/12/lianjiaSpiderDemo/">爬取链家房屋信息-网络爬虫初体验demo</a></h2><div class="post-meta">2016-08-12</div><a data-thread-key="2016/08/12/lianjiaSpiderDemo/" href="/2016/08/12/lianjiaSpiderDemo/#comments" class="ds-thread-count"></a><div class="post-content"><p>　　链家APP以前可以查看房屋的价格变动信息，但新版本后取消了该功能，这样便不利于了解该房源的情况，比如某些房源一路调价3个月仍没卖出，要么不诚心卖，要么有各种问题。<br>　　那么对于想购房的同学，有了目标区域重点关注的房源，如何能够获知其价格变动信息呢？很容易想到，自己轮个爬虫，每天自动爬一次，把房屋信息爬下后存起来，然后展示出来。<br>　　主要的设计思路为：<br>1、    页面的爬取，house info的获取，主要涉及beautifulsoup库的使用；仅爬取指定的若干小区。<br>2、    信息存取：使用MySql数据库，数据库名称lianjiaHouse，包含两张表，两张表通过houseid进行关联：<br>　　a)    houseinfo，保存房屋基本信息，字段包括 价格、朝向、小区、户型、年代、税费等；<br>　　b)    hisprice，保存每天价格信息，字段包括日期、价格等；<br>3、    信息的更新，包括：<br>　　a)    new house的加入：当前爬取的houseid，数据库中没有，则插入记录。 validflag 标记为1，validdate标记为当前日期。并插入hisprice表<br>　　b)    old house失效：每次爬取前，首先将数据库中所有validflag标记为0；若当前爬取的houseid，数据库中有，则validflag 标记为1，validdate标记为当前日期，并更新价格信息；若为同一日期，则覆盖更新价格。这样若非当前爬取的houseid，数据库中保留validflag =0，validdate为最后有效日期，价格信息也仅更新到最后有效日期。<br>4、    信息展示<br>　　a)    前端页面，输入指定小区，可展示所有房源信息和价格变动情况。<br>5、    信息触发发送用户<br>　　当有新上房源或者价格变动时，可触发邮件或短信给用户。      　　</p></div><p class="readmore"><a href="/2016/08/12/lianjiaSpiderDemo/">Read More</a></p></div><div class="post"><h2 class="post-title"><a href="/2016/08/03/推荐系统简介之一（itemCF算法）/">推荐系统简介之一（itemCF算法）</a></h2><div class="post-meta">2016-08-03</div><a data-thread-key="2016/08/03/推荐系统简介之一（itemCF算法）/" href="/2016/08/03/推荐系统简介之一（itemCF算法）/#comments" class="ds-thread-count"></a><div class="post-content"><p>推荐系统简介之一（itemCF算法）<br>　　习惯了今日头条、网易云音乐的推荐，对其背后的推荐原理产生了兴趣，大致了解了一下，主要利用用户上下文信息，比如注册信息、社交好友、时间地点、用户行为（购买、浏览记录等），来进行相关的分析。<br>　　目前比较流行的推荐算法有userCF、itemCF、SVD++等。<br>　　对于电商类网站，基于物品的协同过滤（item-based collaborative filtering）算法是目前业界应用最多的算法，如Amazon等。<br>　　itemCF思想：给用户推荐那些和他们之前喜欢的物品相似的物品。<br>　　(1) 计算物品之间的相似度。<br>　　(2) 根据物品的相似度和用户的历史行为给用户生成推荐列表。<br>　　<br>　　itemCF算法具体过程：<br>　　1、构建M*N矩阵，N为用户数，M为item数，矩阵中数值rji表示用户i对item j的打分项。<br>　　2、计算item i与item j的相似度，可以采用余弦相似度进行计算。<br>　　<img src="../../../../imgs/itemCF/rec_item_similarity_cal0_small.png" alt="余弦相似度定义"><br>　　为增强数值范围的健壮性，通常对每个item的N个user的打分值进行归一化，即每个user的打分值减去该item项的平均打分值。<br>　　为避免未打分的user与item项影响，通常仅取矩阵中的item i与item j的公共用户的打分值进行相似度计算。<br>　　相似度计算时，矩阵中打分数据通常并不进行归一化操作，如下面代码所示，而是直接计算，这时公式需要变形为下式：<br>　　<img src="../../../../imgs/itemCF/rec_item_similarity_cal1_small.png" alt="代码中余弦相似度的计算"><br>　　3、对用户x的待预测各item i，预测打分值，打分值按照最相似的topN个物品的加权打分和进行计算：<br>　　<img src="../../../../imgs/itemCF/rec_item_similarity_cal2_small.png" alt="预测值计算"><br>　　top N可以固定参数N，也可以设置门限，仅取相似度高于门限的参与计算。用movielens 100w数据集测试，N=10左右有较好效果。<br>　　这样就完成了预测，从而可以将预测打分值高于门限的item项推荐展示给用户。<br>　　<br>　　下面代码来自于<a href="https://github.com/justdark/dml" target="_blank" rel="external">justdark/dml</a>，添加了部分注释.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> scipy <span class="keyword">as</span> sp</div><div class="line"><span class="class"><span class="keyword">class</span>  <span class="title">Item_based_C</span>:</span></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,X)</span>:</span></div><div class="line">		self.X=np.array(X)</div><div class="line">		<span class="keyword">print</span> <span class="string">"the input data size is "</span>,self.X.shape</div><div class="line">		self.movie_user=&#123;&#125;</div><div class="line">		self.user_movie=&#123;&#125;</div><div class="line">		self.ave=np.mean(self.X[:,<span class="number">2</span>])</div><div class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> range(self.X.shape[<span class="number">0</span>]):</div><div class="line">			uid=self.X[i][<span class="number">0</span>]</div><div class="line">			mid=self.X[i][<span class="number">2</span>]</div><div class="line">			rat=self.X[i][<span class="number">3</span>]</div><div class="line">			self.movie_user.setdefault(mid,&#123;&#125;)</div><div class="line">			self.user_movie.setdefault(uid,&#123;&#125;)</div><div class="line">			self.movie_user[mid][uid]=rat</div><div class="line">			self.user_movie[uid][mid]=rat</div><div class="line">		self.similarity=&#123;&#125;</div><div class="line">		<span class="keyword">pass</span></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">sim_cal</span><span class="params">(self,m1,m2)</span>:</span></div><div class="line">		self.similarity.setdefault(m1,&#123;&#125;)</div><div class="line">		self.similarity.setdefault(m2,&#123;&#125;)</div><div class="line">		self.movie_user.setdefault(m1,&#123;&#125;)</div><div class="line">		self.movie_user.setdefault(m2,&#123;&#125;)</div><div class="line">		self.similarity[m1].setdefault(m2,<span class="number">-1</span>)</div><div class="line">		self.similarity[m2].setdefault(m1,<span class="number">-1</span>)</div><div class="line"></div><div class="line">		<span class="keyword">if</span> self.similarity[m1][m2]!=<span class="number">-1</span>:</div><div class="line">			<span class="keyword">return</span> self.similarity[m1][m2]</div><div class="line">		si=&#123;&#125;</div><div class="line">		<span class="keyword">for</span> user <span class="keyword">in</span> self.movie_user[m1]:             <span class="comment"># itme m1 and item target m2 ,common user set si =1</span></div><div class="line">			<span class="keyword">if</span> user <span class="keyword">in</span> self.movie_user[m2]:</div><div class="line">				si[user]=<span class="number">1</span></div><div class="line">		n=len(si)</div><div class="line">		<span class="keyword">if</span> (n==<span class="number">0</span>):</div><div class="line">			self.similarity[m1][m2]=<span class="number">1</span></div><div class="line">			self.similarity[m2][m1]=<span class="number">1</span></div><div class="line">			<span class="keyword">return</span> <span class="number">1</span></div><div class="line">		s1=np.array([self.movie_user[m1][u] <span class="keyword">for</span> u <span class="keyword">in</span> si])    <span class="comment"># item j, common user</span></div><div class="line">		s2=np.array([self.movie_user[m2][u] <span class="keyword">for</span> u <span class="keyword">in</span> si])    <span class="comment"># item target mid, common user</span></div><div class="line">		sum1=np.sum(s1)</div><div class="line">		sum2=np.sum(s2)</div><div class="line">		sum1Sq=np.sum(s1**<span class="number">2</span>)</div><div class="line">		sum2Sq=np.sum(s2**<span class="number">2</span>)</div><div class="line">		pSum=np.sum(s1*s2)</div><div class="line">		num=pSum-(sum1*sum2/n)            <span class="comment"># equal to sum[(s1-aveS1)*(s2-aveS2)]</span></div><div class="line">		den=np.sqrt((sum1Sq-sum1**<span class="number">2</span>/n)*(sum2Sq-sum2**<span class="number">2</span>/n))</div><div class="line">		<span class="keyword">if</span> den==<span class="number">0</span>:</div><div class="line">			self.similarity[m1][m2]=<span class="number">0</span></div><div class="line">			self.similarity[m2][m1]=<span class="number">0</span></div><div class="line">			<span class="keyword">return</span> <span class="number">0</span></div><div class="line">		self.similarity[m1][m2]=num/den</div><div class="line">		self.similarity[m2][m1]=num/den</div><div class="line">		<span class="keyword">return</span> num/den</div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">pred</span><span class="params">(self,uid,mid)</span>:</span></div><div class="line">		sim_accumulate=<span class="number">0.0</span></div><div class="line">		rat_acc=<span class="number">0.0</span></div><div class="line">		<span class="keyword">for</span> item <span class="keyword">in</span> self.user_movie[uid]:     <span class="comment"># only fetch rating[item][uid]!=0, avoid cal all item</span></div><div class="line">			sim=self.sim_cal(item,mid)      <span class="comment"># sji, i=mid</span></div><div class="line">			<span class="keyword">if</span> sim&lt;<span class="number">0</span>:<span class="keyword">continue</span></div><div class="line">			<span class="comment">#print sim,self.user_movie[uid][item],sim*self.user_movie[uid][item]</span></div><div class="line">			rat_acc+=sim*self.user_movie[uid][item]   <span class="comment"># weight sum all non-zero items,  Sij*rjx</span></div><div class="line">			sim_accumulate+=sim                       <span class="comment"># sum(sij)</span></div><div class="line">		<span class="comment">#print rat_acc,sim_accumulate</span></div><div class="line">		<span class="keyword">if</span> sim_accumulate==<span class="number">0</span>: <span class="comment">#no same user rated,return average rates of the data</span></div><div class="line">			<span class="keyword">return</span>  self.ave</div><div class="line">		<span class="keyword">return</span> rat_acc/sim_accumulate</div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self,test_X)</span>:</span></div><div class="line">		test_X=np.array(test_X)</div><div class="line">		output=[]</div><div class="line">		sums=<span class="number">0</span></div><div class="line">		bersum=<span class="number">0</span></div><div class="line">		ber=<span class="number">0</span></div><div class="line">		<span class="keyword">print</span> <span class="string">"the test data size is "</span>,test_X.shape</div><div class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> range(test_X.shape[<span class="number">0</span>]):</div><div class="line">			pre=self.pred(test_X[i][<span class="number">0</span>],test_X[i][<span class="number">4</span>])</div><div class="line">			output.append(pre)</div><div class="line">			<span class="comment">#print pre,test_X[i][5]</span></div><div class="line">			sums+=(pre-test_X[i][<span class="number">6</span>])**<span class="number">2</span></div><div class="line">			bersum+=np.abs(pre-test_X[i][<span class="number">7</span>])/test_X[i][<span class="number">8</span>]</div><div class="line">		rmse=np.sqrt(sums/test_X.shape[<span class="number">0</span>])</div><div class="line">		ber = bersum/test_X.shape[<span class="number">0</span>]</div><div class="line">		<span class="keyword">print</span> <span class="string">"the rmse on test data is "</span>,rmse</div><div class="line">		<span class="keyword">print</span> <span class="string">"the precision error percent on test data is "</span>,ber</div><div class="line">		<span class="keyword">return</span> output</div></pre></td></tr></table></figure></p></div><p class="readmore"><a href="/2016/08/03/推荐系统简介之一（itemCF算法）/">Read More</a></p></div><nav class="page-navigator"><a class="extend prev" rel="prev" href="/">Previous</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next</a></nav></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/5G/">5G</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Funny/">Funny</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life/">Life</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML/">ML</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Web/">Web</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Funny/" style="font-size: 15px;">Funny</a> <a href="/tags/LTE/" style="font-size: 15px;">LTE</a> <a href="/tags/NB-IoT/" style="font-size: 15px;">NB-IoT</a> <a href="/tags/MachineLearning/" style="font-size: 15px;">MachineLearning</a> <a href="/tags/DeepLearning/" style="font-size: 15px;">DeepLearning</a> <a href="/tags/crawl/" style="font-size: 15px;">crawl</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Life/" style="font-size: 15px;">Life</a> <a href="/tags/Crab/" style="font-size: 15px;">Crab</a> <a href="/tags/RecSys/" style="font-size: 15px;">RecSys</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/12/11/nnzoo/">神经网络架构大盘点--读Fjodor Van Veen的《neural-network-zoo》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/05/tradespider/">爬虫抓取建委数据--观察存量房成交量变化趋势</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/27/cnnnet/">CNN经典网络模型摘要--AlexNet、ZFnet、GoogleNet、VGG、ResNet</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/20/cnnpara/">神经网络参数优化--基于CNN的验证</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/13/googlenet-inception/">为什么GoogleNet中的Inception Module使用1*1 convolutions?</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/06/gradvanish/">深度神经网络的梯度不稳定问题--梯度消失与梯度爆炸</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/01/actfunpaper/">神经网络不同激活函数比较--读《Understanding the difficulty of training deep feedforward neural networks》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/24/nnpara/">神经网络参数优化</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/18/mlppaper/">回归初心-读《Deep Big Simple Neural Nets Excel on Hand-written Digit Recognition》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/11/datascientistway/">数据科学家路线图</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> Recent Comments</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="http://blog.csdn.net/ziyubiti" title="ziyubiti's csdn blog" target="_blank">ziyubiti's csdn blog</a><ul></ul><a href="https://github.com/ziyubiti" title="ziyubiti's github repository" target="_blank">ziyubiti's github repository</a><ul></ul><a href="null" title="null" target="_blank"></a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">ziyubiti 2006-2017.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a> Theme by<a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> maupassant.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'ziyubiti'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>